{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1012fd3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Foundations of Language Technology 2022/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1587278-ce5d-4bfd-8356-8090d026c017",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8955aa07cdd2a29f843cbe84042d6eef",
     "grade": false,
     "grade_id": "cell-d53919901419608d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Foundations of Language Technology WS 22/23: Homework 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3008f7-1b5a-424e-b772-e9cea80adfec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9912bdd20bd83f1506c4de037ec0ef80",
     "grade": false,
     "grade_id": "cell-b04fc55073bab8d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Please send your solution as a zip-file containing all *.ipynb files that were provided for this homework.(.ipynb). Include comments in your program code to make it easier readable. \n",
    "\n",
    "**Naming template: Group_X_homework_Y.ipynb, Group_X_homework_Y.zip**\n",
    "\n",
    "Please replace X with your group number and Y with the homework number. Submissions that do not follow these rules will not be considered. \n",
    "\n",
    "Please only modify the template in the specified markdown and code cells (e.g. `YOUR CODE / ANSWER / IMPORTS HERE`). \n",
    "Some cells are left blank on purpose. Please do not modify these cells, because they are used to autograde your submission.\n",
    "\n",
    "The deadline for the homework is **Friday, 13/01/2023**. Late submissions will not be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d9df2-87e8-4eb5-98e3-1ed3a6c50d36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR IMPORTS HERE\n",
    "from typing import List, Dict\n",
    "import nltk, random, csv\n",
    "import re\n",
    "from typing import Tuple\n",
    "import math\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796a99e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37bfa5ba4922de3a43fc4a60d0d8fb87",
     "grade": false,
     "grade_id": "cell-1642900816b888c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "If you are using Google Colab, you can mount your Google Drive to access files from there because the file `dataset.csv` which is used in this homework might be too large to upload to Colab.\n",
    "(Google Colab is not required for this homework).\n",
    "\n",
    "\n",
    "To mount your Google Drive, run the following cell and follow the instructions.\n",
    "Save the file `dataset.csv` in your Google Drive and change the path in the following cell to the path where you saved the file.\n",
    "\n",
    "If you need more information about how to use Google Colab with Google Drive, you can find it [here](https://towardsdatascience.com/different-ways-to-connect-google-drive-to-a-google-colab-notebook-pt-1-de03433d2f7a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a94c9c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# GOOGLE COLAB ONLY\n",
    "# remove the # in the following lines to mount your Google Drive if you are using Google Colab, then execute this cell and follow the instructions to mount your Google Drive if you are using Google Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# to access the file dataset.csv, store it in your Google Drive and change the path to the path where you saved the file\n",
    "\n",
    "# path = \"/content/drive/MyDrive/dataset.csv\"\n",
    "\n",
    "# you can use the file like this:\n",
    "# f = open(path, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0f869",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a69c607da3c8af3a9f7dcad8c057e2fd",
     "grade": false,
     "grade_id": "cell-4128867b745bcd00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Part A: Language Identifier (27 points)\n",
    "\n",
    "In this task, you will implement a language identifier. Given a text, it predicts the language that this text is written in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ba5f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b19e7f0509838086adc5d2238bb4b0cd",
     "grade": false,
     "grade_id": "cell-5cfdddfef39cd2f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task A.1: Data for the Language Identifier (9 points)\n",
    "We use the data from `dataset.csv` to train, test, and evaluate the language identifier. The file contains strings of text and their label (the language of the text sample).\n",
    "We will only use samples in French, English, and Swedish for our language identifier.\n",
    "* a) (3 points) Open the CSV file. Only keep the samples that are in French, English, or Swedish and store them in the variable `language_data`. The variable should be a list of tuples, where each tuple contains the text and the language label.\n",
    "\n",
    "*Hint: You should skip the first line of the CSV file, because it contains the column names (header).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602face2-5a06-4fa8-8050-2bb2dbc6ec15",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9e69162a09a1e68ed38cce8d81e0efc",
     "grade": false,
     "grade_id": "cell-3ca8be35979c4e8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = ['French', 'English', 'Swedish']\n",
    "language_data = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580a29a-1b85-4374-86a6-3b50b1c7a6d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "337b251cde5ac5974ad57244c0b9da48",
     "grade": true,
     "grade_id": "cell-8c63e250ecb6a1c1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (2 points)\n",
    "# assert that the number of samples is correct\n",
    "assert len(language_data) == 3000, \"You did not read the correct number of texts.\"\n",
    "# assert list of tuples\n",
    "assert all([isinstance(x, tuple) for x in language_data]), \"You did not read the correct data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81ea5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5acdf1e54947368426b878ef99591132",
     "grade": true,
     "grade_id": "cell-6f884a9a83160e43",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  HIDDEN TESTS, DO NOT MODIFY THIS CELL (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd08d3-16be-4e54-9412-5188931feccb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72dafd20c3cc7180c69e1d16e78a1447",
     "grade": false,
     "grade_id": "cell-5597b55acce67c9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "* b) (4 points) Now we can split the data into a training, dev, and a test set. Write a function `create_dataset` that takes a list of samples and splits it into a training, dev, and a test set. The function should return three lists, where each list contains the samples for the corresponding set. The function should take the following parameters:\n",
    "    * `dataset`: the list of samples (list of tuples)\n",
    "    * `train_ratio`: the ratio of samples that should be in the training set (float)\n",
    "    * `dev_ratio`: the ratio of samples that should be in the dev set (float)\n",
    "    * `test_ratio`: the ratio of samples that should be in the test set (float)\n",
    "    * `random_seed`: the seed for the random number generator\n",
    "Use the random seed to shuffle the samples before splitting them into the different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12afd0-f53a-4b1c-9eb7-3a902c984f4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c37845b79daba10906bda3a0f39508ab",
     "grade": false,
     "grade_id": "cell-0e3050fa05678525",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset: list, train_ratio: float, dev_ratio: float, test_ratio: float, random_seed: int) -> (\n",
    "        list, list, list):\n",
    "    \"\"\"\n",
    "    Splits the dataset into a training, dev, and a test set.\n",
    "    :param dataset: the list of samples (list of tuples)\n",
    "    :param train_ratio: the ratio of samples that should be in the training set (float)\n",
    "    :param dev_ratio: the ratio of samples that should be in the dev set (float)\n",
    "    :param test_ratio: the ratio of samples that should be in the test set (float)\n",
    "    :param random_seed: the seed for the random number generator\n",
    "    :return: three lists, where each list contains the samples for the corresponding set\n",
    "    \"\"\"\n",
    "    train_set, dev_set, test_set = [], [], []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9505062-78d7-4adb-ad7b-5fa9e3d5916c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "train_set, dev_set, test_set = create_dataset(dataset=language_data, train_ratio=0.7, dev_ratio=0.15, test_ratio=0.15,\n",
    "                                              random_seed=seed)\n",
    "print(\"Train={}, Dev={}, Test={}\".format(len(train_set), len(dev_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fd9ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51b05fbd97600498a47303a6be3bdad8",
     "grade": true,
     "grade_id": "cell-bf930a1cbb455542",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (3 points)\n",
    "# assert train set ratio is close to 0.7\n",
    "assert math.isclose(len(train_set) / len(language_data), 0.7,\n",
    "                    abs_tol=1e-8), \"The ratio of samples in the training set is incorrect.\"\n",
    "# assert dev set ratio is close to 0.15\n",
    "assert math.isclose(len(dev_set) / len(language_data), 0.15,\n",
    "                    abs_tol=1e-8), \"The ratio of samples in the dev set is incorrect.\"\n",
    "# assert test set ratio is close to 0.15\n",
    "assert math.isclose(len(test_set) / len(language_data), 0.15,\n",
    "                    abs_tol=1e-8), \"The ratio of samples in the test set is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f6c9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35d02ab5d63b85782e67d387c0882b47",
     "grade": true,
     "grade_id": "cell-11c329b30957f9ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2eed4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bc28be8bf0a87426f00b7d96eaaa4bb",
     "grade": false,
     "grade_id": "cell-129fda1a44eccac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "* c) (2 points) Before any NLP task, we should perform pre-processing to match the text format as in the data used to build the model. In our case, we need to tokenize the input text and lowercase the tokens. Implement a function `normalize_text` that takes a text and returns a list of strings. The function should take the following parameter:\n",
    "    * `text`: the text to be normalized (string)\n",
    "\n",
    "    The function should return a list of strings where each string is a token from the text. The tokens should be lowercase. You should use the `nltk.tokenize.word_tokenize` function to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30195a5b",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1ff83d9c2860c0b6b7176476c6ef9ff",
     "grade": true,
     "grade_id": "cell-997d13671e47cdbd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_text(text) -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize the text by tokenizing and lowercasing the tokens.\n",
    "    :param text: the text to be normalized (string)\n",
    "    :return: a list of strings where each string is a token from the text (list of strings)\n",
    "    \"\"\"\n",
    "    results: List[str] = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769ca65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13f29fad30a3652244ac956a78024b83",
     "grade": true,
     "grade_id": "cell-72fb1cab12e2ecf1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (2 points)\n",
    "# assert the function returns the correct tokens\n",
    "assert normalize_text(\"Hello   World!\") == [\"hello\", \"world\", \"!\"], \"The function returns the incorrect tokens.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6998c5-9406-4edb-a7da-2abec5749c28",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ac2933dc326195d8bc2f72df8356c3a",
     "grade": false,
     "grade_id": "cell-1a868ee8f6bb2236",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task A.2: Language Identifier Using Character Bigram Language Model (12 points)\n",
    "\n",
    "Now you will implement the language identifier, i.e. a function that takes a given text and outputs the language it thinks the text is written in.\n",
    "The function should base its decision on the frequency of character bigrams in each language.\n",
    "\n",
    "*Remember: A character bigram is a sequence of two characters. For example, the text \"Hello\" contains the following character bigrams: \"He\", \"el\", \"ll\", \"lo\".\n",
    "The language identifier might decide that the text is written in English, because the character bigram \"ll\" occurs more often in English than in French or Swedish.*\n",
    "\n",
    "We will build our language model based on the samples we retrieved from the dataset in Task A.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491782d7-3508-4050-b611-ff3a67c75501",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0473325bea10b3ee870c951fd14d566",
     "grade": false,
     "grade_id": "cell-cef9acdf4906f211",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "* a) (3 points) Implement a function `build_language_models(languages, words)`. The function should take a list of languages and a list of words and build a language model for each language. The function should return a conditional frequency distribution (nltk.ConditionalFreqDist) that contains the frequency of character bigrams for each language. The function should take the following parameters:\n",
    "    * `languages`: the list of languages (list of strings) for which the language model should be built\n",
    "    * `words`: a dictionary that maps each language to a list of words (list of strings) in that language, e.g. `{\"English\": [\"Hello\", \"World\"], \"French\": [\"Bonjour\", \"Monde\"]}`\n",
    "\n",
    "    The returned conditional frequency distribution should have the following structure:\n",
    "    * The conditions of the conditional frequency distribution are the languages.\n",
    "    * The values are the **lower cased** character bigrams found in `words[language]` as tuples, e.g. `(\"h\", \"e\"), (\"e\", \"l\"), (\"l\", \"l\"), (\"l\", \"o\")`.\n",
    "\n",
    "Hints:\n",
    "- Use `nltk.bigrams` to get the character bigrams from a word. Make sure to convert the word to lower case before getting the character bigrams.\n",
    "- Use the function `nltk.ConditionalFreqDist().update(...)` to update the conditional frequency distribution with the character bigrams for a given language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904d6f8-425a-4f4a-b88c-e285f150d933",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c600d9afcfd976808645dffc0067ee",
     "grade": false,
     "grade_id": "cell-24081bb359d5444a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_language_models(languages: List[str], words: Dict) -> nltk.ConditionalFreqDist:\n",
    "    \"\"\"\n",
    "    Return conditional frequency distribution where:\n",
    "        - the languages are the conditions\n",
    "        - the values are the lower cased character bigrams\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "    languages: list of strings\n",
    "        list of language names\n",
    "    words: dict of lists of strings\n",
    "        dictionary where the keys are the language names and the values are the words in the language\n",
    "    \"\"\"\n",
    "    cfd = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85342c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f556f17593176af01ccf3dde9576a333",
     "grade": true,
     "grade_id": "cell-baa966b7ae4e550e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (2 points)\n",
    "# assert the conditional frequency distribution has the correct structure\n",
    "cfd = build_language_models(languages=[\"English\", \"French\", \"Swedish\"],\n",
    "                            words={\"English\": [\"Hello\", \"World\"],\n",
    "                                   \"French\": [\"Bonjour\", \"Monde\"],\n",
    "                                   \"Swedish\": [\"Hej\", \"Världen\"]})\n",
    "assert cfd.conditions() == [\"English\", \"French\",\n",
    "                            \"Swedish\"], \"The conditional frequency distribution has the wrong conditions.\"\n",
    "\n",
    "# assert the format of the values is correct\n",
    "assert all([isinstance(cfd[language].most_common(1)[0][0], tuple) for language in\n",
    "            cfd.conditions()]), \"The values of the conditional frequency distribution are incorrect.\"\n",
    "\n",
    "# assert the most common character bigram is correct for French\n",
    "assert cfd[\"French\"].most_common(1)[0][0] == (\"o\", \"n\"), \"The most common character bigram for French is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86e688",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "659e80c55604444cdb1abe91701c947b",
     "grade": true,
     "grade_id": "cell-97ace448f59cc231",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076959e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d7bbc64ffc319fc59ee05df4dfeda68",
     "grade": false,
     "grade_id": "cell-e22de38281c564a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "* b) (4 points) To create the language models, we should be able to access the words from the dataset by language. Write a function `get_words_by_language` that takes a list of samples and returns a dictionary that maps languages to words. The function should take the following parameters:\n",
    "    * `dataset`: the list of samples (list of tuples) containing the texts and their language\n",
    "    * `languages`: the list of languages (list of strings) for which the texts should be returned\n",
    "\n",
    "    The returned dictionary should have the following structure:\n",
    "    * The keys are the languages.\n",
    "    * The values are all the words (list of strings) from the samples for the corresponding language as a single list.\n",
    "    The dictionary should only contain the languages that are specified in the parameter `languages`.\n",
    "\n",
    "    The function should use the function `normalize_text` to normalize (tokenize and lowercase) the text of each sample before splitting it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532c697",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6db4dbc0e3e07d40161e59511b1d853",
     "grade": false,
     "grade_id": "cell-72ba552462be546b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words_by_language(languages: list, dataset: list) -> dict:\n",
    "    \"\"\"\n",
    "    Return a dictionary that maps languages to words.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "    languages: list of strings\n",
    "        list of language names that should be in the returned dictionary\n",
    "    dataset: list of tuples\n",
    "        list of samples containing the texts and their language\n",
    "    \"\"\"\n",
    "    words_by_language = dict()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return words_by_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163965c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "367246df23bb9a7122d970b5c2850ea2",
     "grade": true,
     "grade_id": "cell-6e20730cf9816883",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (2 points)\n",
    "# assert the dictionary has the correct structure\n",
    "words_by_language = get_words_by_language(languages=[\"English\", \"French\", \"Swedish\"],\n",
    "                                          dataset=[(\"Hello World!\", \"English\"),\n",
    "                                                   (\"Bonjour Monde!\", \"French\"),\n",
    "                                                   (\"Hej Världen!\", \"Swedish\")])\n",
    "assert words_by_language == {\"English\": [\"hello\", \"world\", \"!\"], \"French\": [\"bonjour\", \"monde\", \"!\"],\n",
    "                             \"Swedish\": [\"hej\", \"världen\", \"!\"]}, \"The dictionary has the incorrect structure.\"\n",
    "# assert the dictionary contains the correct words\n",
    "assert words_by_language[\"English\"] == [\"hello\", \"world\", \"!\"], \"The dictionary contains the incorrect words.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f775",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfdb16461467d2cc14fd7bb593fd1c2a",
     "grade": false,
     "grade_id": "cell-326c5cc5bef58ce2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Now we can build the language models. We use the function `get_words_by_language` to get the texts for the languages English, French, and Swedish from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba6b7f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "language_samples_train = get_words_by_language([\"English\", \"French\", \"Swedish\"], train_set)\n",
    "print(\"Number of English words: {}\".format(len(language_samples_train[\"English\"])))\n",
    "print(\"Number of French words: {}\".format(len(language_samples_train[\"French\"])))\n",
    "print(\"Number of Swedish words: {}\".format(len(language_samples_train[\"Swedish\"])))\n",
    "print(\"Number of words in total: {}\".format(\n",
    "    len(language_samples_train[\"English\"]) + len(language_samples_train[\"French\"]) + len(\n",
    "        language_samples_train[\"Swedish\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab841d8-ce0c-48e2-b665-328008666552",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "616659093b39ea5d94e1e9d1e4bc40f7",
     "grade": true,
     "grade_id": "cell-a7c2708a2b5f3d99",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (2 points)\n",
    "# assert the dictionary has likely correct numbers of words\n",
    "assert len(language_samples_train[\"English\"]) > 48000, \"Number of English words is incorrect!\"\n",
    "assert len(language_samples_train[\"French\"]) > 46000, \"Number of English words is incorrect!\"\n",
    "assert len(language_samples_train[\"Swedish\"]) > 34000, \"Number of English words is incorrect!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e553d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7d9fbc0c13b9709de820afd515862fa",
     "grade": false,
     "grade_id": "cell-64652a6fcc493389",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    " * c) (5 points) Use the function `build_language_models` to build the language models for the languages English, French, and Swedish (`languages`) based on the training set `language_samples_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813b603-50ea-4ffd-bb86-9ad8bf68f1b4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd46a4c670e13335bf3d2a13904db2bb",
     "grade": false,
     "grade_id": "cell-5cd498ca1d105571",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the language model\n",
    "language_model_cfd = None\n",
    "# Call the function `build_language_models` with correct arguments from above\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# print the conditions of the conditional frequency distribution\n",
    "language_model_cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed021c02-0f7e-46b6-a261-b939075ef97e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f2e15062c0f0082ab7ef2336204b6eb",
     "grade": true,
     "grade_id": "cell-6ac937bb5255a934",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (2 points)\n",
    "# assert the conditional frequency distribution has the correct structure\n",
    "assert len(\n",
    "    language_model_cfd.conditions()) == 3, \"The number of conditions in the conditional frequency distribution is incorrect.\"\n",
    "# assert the conditional frequency distribution has the correct values\n",
    "assert language_model_cfd['French'].freq(\n",
    "    ('é', 'c')) > 0.001, \"The frequency of the character bigram 'éc' in French is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856aa9d2-86bd-46e1-b983-fd38007c2c27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "369be002ab3880824bc25b7cdadd8ddb",
     "grade": true,
     "grade_id": "cell-575b19f420b60530",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8c6b4-3d74-43a0-9433-05b1b5d42229",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5f787ad9adbe3d7ab56de49a6822cf7",
     "grade": false,
     "grade_id": "cell-2232b65d3cd8894d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task A.3: Predicting Languages (6 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabef7b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eaea20738cbf578402afe6aa59bf5b6",
     "grade": false,
     "grade_id": "cell-69c00eef1a1c5dec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Implement a function `predict_language(language_model_cfd,text)` that takes a conditional frequency distribution of character bigrams and a text and predicts the language of the text. The function should return the language with the highest probability. The function should take the following parameters:\n",
    "    * `language_model_cfd`: the conditional frequency distribution of character bigrams\n",
    "    * `text`: the text for which the language should be predicted\n",
    "\n",
    "*Hints:*\n",
    "   * In this task, we utilize the frequency of character bigrams in the language to calculate the language matching score.\n",
    "   * The language matching score is the sum of the frequency of the character bigrams in the text.\n",
    "   * Calculate the language matching score for each language in the conditional frequency distribution and return the language with the highest score.\n",
    "   * Formula for one language: $score_{language} = \\sum_{bigram \\in text} freq_{language}(bigram)$\n",
    "   * where $freq_{language}(bigram)$ is the frequency of the character bigram in this language\n",
    "   * Return: $language \\in \\text{language_model_cfd.conditions()}$ with the highest score\n",
    "   * If the language matching score is the same for two or more languages, return the language that appears first in the list of languages in the conditional frequency distribution.\n",
    "   * Each occurrence of a character bigram contributes to the score, thus the bigrams, that are indicative of a particular language (have high frequency) and appear often in the text, contribute the most to the final score for each language.\n",
    "   * The higher the score the more likely is that the text is written in that language.\n",
    "   * You can use the `nltk.ngrams` function to generate character bigrams from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d08bb3",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a797ebef012172585a2081a068e14e6e",
     "grade": false,
     "grade_id": "cell-9c7226f1f0d99556",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_language(language_model_cfd: nltk.ConditionalFreqDist, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Predict/Guess the language for the given text\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    language_model_cfd:\n",
    "        dict-like object (ConditionalFreqDist) that maps languages to character bigrams\n",
    "    text: string\n",
    "        a given text to be predicted\n",
    "    return: string\n",
    "        Name of the most likely language from the given text (key of the language_model_cfd)\n",
    "    \"\"\"\n",
    "    max_score = 0\n",
    "    most_likely_language = \"\"\n",
    "\n",
    "    # Normalize the text before predicting the language\n",
    "    words = normalize_text(text)\n",
    "\n",
    "    # Calculate the language matching score for each language\n",
    "    for language in language_model_cfd.conditions():\n",
    "        score = 0\n",
    "        # Calculate the score for each language and keep track of the language with the highest score\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Return the language with the highest score\n",
    "    return most_likely_language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f8168-26a9-4e18-bda7-30886f70611c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c2085bad1c3da1e7f36663ebbe5d6ea",
     "grade": true,
     "grade_id": "cell-7f7c0b8f741ea463",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TEST (4 points)\n",
    "# assert the function returns the correct language\n",
    "text1 = \"Du gamla, Du fria, Du fjällhöga nord Du tysta, Du glädjerika sköna! Jag hälsar Dig, vänaste land uppå jord, Din sol, Din himmel, Dina ängder gröna.\"\n",
    "assert predict_language(language_model_cfd, text1) == \"Swedish\", \"The prediction for text1 is incorrect.\"\n",
    "\n",
    "test_2 = \"Tous les hommes naissent égaux. Le Créateur nous a donné des droits inviolables, le droit de vivre, le droit d'être libre et le droit de réaliser notre bonheur.\"\n",
    "assert predict_language(language_model_cfd, test_2) == \"French\", \"The prediction for text2 is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50386d-b574-4b72-aa05-30c5b46b933f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0440ab3d91a8e808da0785f7bbfed7d",
     "grade": true,
     "grade_id": "cell-9caa09712207f41a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff06b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print the samples for which the language prediction is incorrect\n",
    "for text, language in test_set:\n",
    "    predicted_language = predict_language(language_model_cfd, text)\n",
    "    predicted_language = predict_language(language_model_cfd, text)\n",
    "    if predicted_language != language:\n",
    "        print(\"Predicted language: {}, Actual language: {}\".format(predicted_language, language))\n",
    "        print(text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3c981-dd72-4733-82fa-d5150ed36635",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da918a82f4c2f75e47203abe5669bd29",
     "grade": false,
     "grade_id": "cell-870e7c096cfe6367",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Part B: Language Identifier using Naive Bayes (31 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437256de-0feb-4747-8984-e11ceda931d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cf7cbbc41beeff1f1840ca17d028c7e",
     "grade": false,
     "grade_id": "cell-f89ddcb5fbdabdcb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task B.1: Create a feature extractor function (10 points)\n",
    "\n",
    "Now we will use the Naive Bayes classifier to predict the language of a text. We will use a different feature extractor function to create features for the Naive Bayes classifier.\n",
    "\n",
    "The function `extract_features()` should take a text and return a dictionary of features. The features should be\n",
    "* `avg_vowels`: the average number of vowels [a, e, i, o, u, y] in the text\n",
    "* `avg_word_length`: the average word length in the text\n",
    "* `num_accented_chars`: the **total** number of accented characters [á, é, í, ó, ú] in the text\n",
    "* `num_umlaut_chars`: the **total** number of umlaut characters [ä, ö, ü] in the text\n",
    "* `num_punctuation_chars`: the **total** number of punctuation characters [. , ! ? ; :] in the text\n",
    "* `num_uppercase_chars`: the **total** number of uppercase characters in the text\n",
    "* `num_consecutive_vowels`: the **total** number of consecutive vowels in the text (e.g. \"aeiou\" has 5 consecutive vowels)\n",
    "* `num_consecutive_consonants`: the **total** number of consecutive consonants in the text (e.g. \"bcdfghjklmnpqrstvwxyz\" has 21 consecutive consonants)\n",
    "* **all lowercased character bigrams** in the text (the presence of a character trigram can be indicated by setting the value of the feature to `True` or `1`) (e.g. \"hello\" has the character bigrams \"he\", \"el\", \"ll\", \"lo\")\n",
    "\n",
    "\n",
    "Hint: You can use the `nltk.tokenize.word_tokenize` function to tokenize the text where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464fbb6-db1f-4067-8e24-bd63c311535c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For reference\n",
    "accented_chars = u\"áéíóú\"\n",
    "umlaut_chars = u\"äöü\"\n",
    "vowels = \"aeiouy\"\n",
    "consonants = \"bcdfghjklmnpqrstvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce7447-a867-497e-95ce-9d6855fed497",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cd56f04b27d625f6b2205d5b4007797",
     "grade": false,
     "grade_id": "cell-b28c5e712ce5991f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract features from the given text\n",
    "    text: string\n",
    "        a given text to extract features from\n",
    "    return: dict\n",
    "        a dictionary of features\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d2eda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbea0f7261ee70bd9bd31aac7d495a42",
     "grade": true,
     "grade_id": "cell-efda788adad5200b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (5 points)\n",
    "example = \"Bonjour petite fille\"\n",
    "example_features = extract_features(example)\n",
    "# assert the features contain some of the expected features\n",
    "assert \"num_accented_chars\" in example_features, \"The feature 'num_accented_chars' is missing.\"\n",
    "assert \"num_umlaut_chars\" in example_features, \"The feature 'num_umlaut_chars' is missing.\"\n",
    "assert \"num_punctuation_chars\" in example_features, \"The feature 'num_punctuation_chars' is missing.\"\n",
    "\n",
    "# assert avg_vowels is close to 0.4\n",
    "assert math.isclose(example_features[\"avg_vowels\"], 0.4, rel_tol=1e-2), \"The value of 'avg_vowels' is incorrect.\"\n",
    "# assert avg_word_length is close to 6\n",
    "assert math.isclose(example_features[\"avg_word_length\"], 6, rel_tol=1e-2), \"The value of 'avg_word_length' is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85633d81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "981cbd56da4422ec53e4fe487dd44baa",
     "grade": true,
     "grade_id": "cell-1c2f2d406248ed31",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (2 points)\n",
    "hello_test = \"Hello!\"\n",
    "hello_features = extract_features(hello_test)\n",
    "# assert the character bigrams are present as features\n",
    "assert hello_features[\"he\"], \"The feature 'he' is missing.\"\n",
    "assert hello_features[\"el\"], \"The feature 'el' is missing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568305d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e248f253f44882f58ade4a4cfc8b258",
     "grade": true,
     "grade_id": "cell-a0f87abaac0d5fd8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (3 points)\n",
    "# The tests check `num_uppercase_chars`, `num_consecutive_vowels`, `num_consecutive_consonants`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4b046",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3eb1fc2e8fb9410d895636c030e7816",
     "grade": false,
     "grade_id": "cell-5390dc293966eba7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task B.2: Train a Naive Bayes classifier (7 points)\n",
    "* a) (4 points) Prepare the train and test sets for the Naive Bayes classifier. The feature set should be a list of tuples where the first element is the feature dictionary and the second element is the language label. The train set should be created using the texts and language labels in the `train_set` variable from task A.1. The test set should be created using the texts and language labels in the `test_set` variable from task A.1. Store the train and test feature sets in the `train_features` and `test_features` variables respectively.\n",
    "\n",
    "\n",
    "    *Hint: You can use the `extract_features` function to create the feature dictionary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ba955-e3bd-4805-bb76-679dbeb2e1a4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e973c668fc892e44a0f9c1e3cffc58e8",
     "grade": false,
     "grade_id": "cell-bb6d3b88637262c2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features, test_features = [], []\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172f094",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "070533206d073ac5a2ede8774b780175",
     "grade": true,
     "grade_id": "cell-0e7881ece4a54e78",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (4 points)\n",
    "# assert the train data contains the expected number of texts\n",
    "assert len(train_features) == 2100, \"The train data does not contain the expected number of texts.\"\n",
    "# assert the test data contains the expected number of texts\n",
    "assert len(test_features) == 450, \"The test data does not contain the expected number of texts.\"\n",
    "# assert the train_features is  in the expected format\n",
    "assert isinstance(train_features[0], tuple), \"The train data is not in the expected format.\"\n",
    "# assert the train data contains some of the expected features\n",
    "assert \"avg_vowels\" in train_features[0][0], \"The feature 'avg_vowels' is missing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c6aff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "832830776bc283e708091ae497405b93",
     "grade": false,
     "grade_id": "cell-3b35116426f6afc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "* b) (3 points) Train the Naive Bayes classifier using the training data and evaluate the classifier using the test data. Print the accuracy of the classifier. Print the 10 most informative features of the classifier.\n",
    " Use the `classifier` variable to store the trained classifier.\n",
    "\n",
    "    Hint: You can use the `nltk.classify.accuracy` function to evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e783200",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebf9e06716a8c81dc3917d6798cf7d61",
     "grade": false,
     "grade_id": "cell-66e817ed668ec94f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = None\n",
    "accuracy = 0.0\n",
    "top10_most_informative_features = []\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Top 10 most informative features:\")\n",
    "for feature in top10_most_informative_features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0d5ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4eb8cb4c1d192e43f16c987de486f394",
     "grade": true,
     "grade_id": "cell-4623f668b7b772ec",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (1 point)\n",
    "# assert the classifier is trained\n",
    "assert classifier, \"The classifier is not trained.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ffcc5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11102a1fc37e008f4ee26f69e3ff891",
     "grade": true,
     "grade_id": "cell-a381f2d718255381",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (1 point)\n",
    "# assert the accuracy is correct\n",
    "assert accuracy > 0.98, \"The accuracy is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fbf047",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "169123dbd07aa1f09e5b179653a4c899",
     "grade": true,
     "grade_id": "cell-e096cf5342b3af2b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d44a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f7b84c2c1cd67320b2cbfdeed815392",
     "grade": false,
     "grade_id": "cell-a6550e6b7aea93ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task B.3: Evaluate the classifiers (14 points)\n",
    "\n",
    "We want to evaluate both the Naive Bayes classifier and the cfdist classifier from task A.3. We will use the same test set for both classifiers.\n",
    "The performance of the classifiers is measured using the following metrics:\n",
    "* **Accuracy**: the percentage of correctly classified texts\n",
    "* **F1 scores**: the harmonic mean of the precision and recall\n",
    "\n",
    "Implement a function `evaluate_predictions` that takes as input a list of predictions and a list of gold labels and returns the accuracy, and the F1 scores for each language. The function should return a tuple of the form `(accuracy, f1_scores)`, where `accuracy` is a float and `f1_scores` is a dictionary with language labels as keys and F1 scores as values.\n",
    "The function should take the following parameters:\n",
    "* `predictions`: a list of predicted language labels\n",
    "* `gold_labels`: a list of gold language labels (the correct language labels as in the test set)\n",
    "\n",
    "\n",
    "The function should return the following values:\n",
    "* `accuracy`: a float representing the accuracy of the predictions\n",
    "* `f1_scores`: a dictionary with language labels as keys and F1 scores as values\n",
    "\n",
    "\n",
    "The function should work for both the Naive Bayes classifier and the cfdist classifier from task A.3.\n",
    "\n",
    "**Hints:**\n",
    "    - The accuracy is the percentage of texts for which the language prediction is correct.\n",
    "    - The F1 score is computed as the harmonic mean of the precision and recall for each language.\n",
    "    - The formula for the F1 score is: $F1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$\n",
    "    - The precision is the percentage of texts for which the language prediction is correct and the language of the text is the predicted language.\n",
    "    - The recall is the percentage of texts for which the language of the text is the predicted language.\n",
    "    - You should not use any external libraries (e.g. sklearn, nltk) to compute the metrics, but use the formulas above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a83337-9b6a-49fc-9296-487b0ff963fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8147fdb720f88e74336be932ff25c46a",
     "grade": false,
     "grade_id": "cell-3fe07fc6398ed45e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions: List[str], gold_labels: List[str]) -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate the predictions of a classifier.\n",
    "    :param predictions: the predictions of the classifier (a list of language labels)\n",
    "    :param gold_labels: the gold labels (a list of language labels), i.e. the correct language labels as in the test set\n",
    "    :return: a tuple of the form (accuracy, f1_scores), where accuracy is a float and f1_scores is a dictionary with language labels as keys and F1 scores as values\n",
    "    \"\"\"\n",
    "    accuracy = 0.0\n",
    "    f1_scores = {}\n",
    "    for language in set(gold_labels):\n",
    "        f1_scores[language] = 0.0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb62953",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparison of the performance of the Naive Bayes classifier with the performance of the cfd model from task A\n",
    "\n",
    "# predictions on the test set of the Naive Bayes classifier\n",
    "# uses the test_features variable from task B.2\n",
    "nb_predictions = [classifier.classify(features) for features, language in test_features]\n",
    "accuracy_nb, f1_scores_nb = evaluate_predictions(nb_predictions, [l for f, l in test_features])\n",
    "\n",
    "# predictions on the test set of the cfdist classifier\n",
    "# uses the test_set variable from task A\n",
    "cfdist_predictions = [predict_language(language_model_cfd=language_model_cfd, text=t) for t, l in test_set]\n",
    "accuracy_cfd, f1_scores_cfd = evaluate_predictions(cfdist_predictions, [l for t, l in test_set])\n",
    "\n",
    "print(f\"Accuracy of the Naive Bayes classifier: {accuracy}\")\n",
    "print(f\"Accuracy of the cfd model: {accuracy_cfd}\")\n",
    "print(f\"F1 scores of the Naive Bayes classifier: {f1_scores_nb}\")\n",
    "print(f\"F1 scores of the cfd model: {f1_scores_cfd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef733ba9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d2ed1b00e1154760d0abedced7fe660",
     "grade": true,
     "grade_id": "cell-ca2a4a5a59d7aea3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (2 points)\n",
    "fake_test_1 = [\"English\", \"English\", \"English\", \"English\", \"English\", \"English\", \"English\", \"English\", \"English\",\n",
    "               \"English\"]\n",
    "test_acc, test_f1 = evaluate_predictions(fake_test_1, fake_test_1)\n",
    "\n",
    "# assert that the accuracy in evaluate_predictions is correct\n",
    "assert test_acc == 1.0, \"The accuracy in evaluate_predictions is incorrect.\"\n",
    "# assert that the F1 score in evaluate_predictions is correct\n",
    "assert test_f1[\"English\"] == 1.0, \"The F1 score in evaluate_predictions is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900d877",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e242667c3a46a56e45c41af922acfd87",
     "grade": true,
     "grade_id": "cell-2f8d5c60ba91ba36",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (2 points)\n",
    "# assert the function returns the expected values for the Naive Bayes classifier\n",
    "nb_accuracy, nb_f1_scores = evaluate_predictions(nb_predictions, [l for f, l in test_features])\n",
    "assert nb_accuracy > 0.93, \"The accuracy is incorrect.\"\n",
    "assert nb_f1_scores[\"English\"] > 0.93, \"The F1 score for the English language is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98690181",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "127410fc76a92817af79dd5f21b2865f",
     "grade": true,
     "grade_id": "cell-e50f3791f378eb78",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (2 points)\n",
    "# assert the function returns the expected values for the cfd model\n",
    "cfdist_accuracy, cfdist_f1_scores = evaluate_predictions(cfdist_predictions, [l for t, l in test_set])\n",
    "assert cfdist_accuracy > 0.95, \"The accuracy is incorrect.\"\n",
    "assert cfdist_f1_scores[\"English\"] > 0.95, \"The F1 score for the English language is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af2c36-bc8d-468e-98af-2f226c7155e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b628c3f3794a2be9a0ab5ec52653082",
     "grade": true,
     "grade_id": "cell-1c4b20e9efe81a26",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PUBLIC TESTS (2 points)\n",
    "# assert the performance of Naive Bayes better than CFDist\n",
    "assert nb_accuracy > cfdist_accuracy, \"NB should work better than CFDist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d5d99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f587a376a18670d6f5435e7d99de851",
     "grade": true,
     "grade_id": "cell-b9e0750f8d3fb15a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f054890",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18430c534e40fd4b708d3d0fe0c19669",
     "grade": true,
     "grade_id": "cell-bb9bec3cd1cddee7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (2 points)\n",
    "# Evaluate the NB classifier on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ec6f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "905b9bfe281f25bcca27ce18767d163b",
     "grade": true,
     "grade_id": "cell-bbbc1021dafc2d3f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN TESTS, DO NOT MODIFY THIS CELL (2 points)\n",
    "# evaluate the cfd model on the dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f170f-6982-4375-9063-58b1704f397a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a1ec954c10bc8190b5ed5c843f88335",
     "grade": false,
     "grade_id": "cell-c780b6536a270525",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Part C: Chatbot that identifies your language (not graded)\n",
    "Finally, we show case the use of these classifiers in a chatbot. \n",
    "The chatbot can identify the language of the user's input. The chatbot asks the user to enter a text and then prints the language of the text. The chatbot asks the user for texts three times or until the user enters the text `exit`.\n",
    "\n",
    "For your interests, you can play around with different types of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52f228-76e6-47c2-8954-07c6d821b78d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def respond_with_predicted_language(language):\n",
    "    \"\"\"\n",
    "    Prints a response to the user based on the predicted language\n",
    "    Parameters\n",
    "    ----------\n",
    "    language: str\n",
    "        the predicted language\n",
    "    \"\"\"\n",
    "    return f\"You're speaking {language}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a6749-4fc0-4e84-9eab-45b4bec327b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello, please speak to me, I can guess the language you speak. (only in English, French, Swedish)\")\n",
    "# while loop that asks the user for a text and prints the predicted language three times\n",
    "for i in range(3):\n",
    "    text = input(\"Please enter a text: \")\n",
    "    if text == \"exit\":\n",
    "        break\n",
    "    features = extract_features(text)\n",
    "    language = classifier.classify(features)\n",
    "    # output the predicted language\n",
    "    print(respond_with_predicted_language(language))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (folt_env)",
   "language": "python",
   "name": "folt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
