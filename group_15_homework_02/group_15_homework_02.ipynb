{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60887a01b12bbce903a6513ee29963aa",
     "grade": false,
     "grade_id": "A_",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Foundation of Language Technolofy WS 22/23: Homework 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff73247b9e4c3076e5c60569600f51b2",
     "grade": false,
     "grade_id": "cell-f25090e143b8cfbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please send your solution as a zip-file containing all *.ipynb files that were provided for this homework.(.ipynb). Include comments in your program code to make it easier readable. \n",
    "\n",
    "**Naming template: Group_X_homework_Y.ipynb, Group_X_homework_Y.zip**\n",
    "\n",
    "Please replace X with your group number and Y with the homework number. Submissions that do not follow these rules will not be considered. \n",
    "\n",
    "Please only modify the template in the specified markdown and code cells (e.g. `YOUR CODE / ANSWER / IMPORTS HERE`). \n",
    "Some cells are left blank on purpose. Please do not modify these cells, because they are used to autograde your submission.\n",
    "\n",
    "The deadline for the homework is **Friday, 02/12/2022**. Late submissions will not be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e20f159d2f3a43449d4d15608e5892c",
     "grade": false,
     "grade_id": "cell-082d57e76e5dea94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import type hint\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6844b0043d684f6d5a6f26c3dd7b395e",
     "grade": false,
     "grade_id": "A",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part A: NLTK (25 points)\n",
    "\n",
    "We will discuss more NLTK functions in this homework. It is recommended to use the NLTK's conditional frequency distribution (`ConditionalFreqDist`) and list comprehension . However, you can implement your solutions without these built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a293ea19d59c5e3c8723b983b901c91b",
     "grade": false,
     "grade_id": "A-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### A.1 Sentiment in opnion lexicon (4 points)\n",
    "The NLTK’s “`opinion_lexicon`” corpus contains roughly 7,000 English words that are classified as having either\n",
    "positive or negative sentiment. The positive and negative words of the corpus can be accessed via the functions\n",
    "`opinion_lexicon.positive()` and `opinion_lexicon.negative()`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ea2b1715030dea8659871d04d05041b",
     "grade": false,
     "grade_id": "cell-bef9a96b42b5a447",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "## Download 'opinion_lexicon' if you haven't already\n",
    "# nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fea077d1e9e43fa7c870b6fec5bb1185",
     "grade": false,
     "grade_id": "cell-9dc31840bdd2b7fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Try it yourself\n",
    "# This will returns a lexicon of positive words/tokens\n",
    "# It will only print the first 5 words, and \"...\" indicates more words in the lexicon\n",
    "opinion_lexicon.positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f46d51fc18bc9e956b085c43e59913a",
     "grade": false,
     "grade_id": "cell-dd50f0572b68a86b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Same for negative words\n",
    "opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "061d7d8ec128ce8764f7504d6b815f59",
     "grade": false,
     "grade_id": "A-1-1_",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1) Count with conditions (2 points)\n",
    "\n",
    "Your task: Use the whole `opinion_lexicon` corpus to count number of negative and positive words starting with `'dis'`\n",
    "\n",
    "Hint: You can go through each word in the lexicon using `for ... in ...` as in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a46b7609b9f85a36ae8ede2016eb719d",
     "grade": false,
     "grade_id": "A-1-1-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def word_start_with_dis() -> (int, int):\n",
    "    \"\"\"\n",
    "    Return the number of negative and positive words starting with 'dis', respectively in opinion_lexicon corpus\n",
    "    \"\"\"\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    ### Count positive words start with \"dis\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### Count negative words start with \"dis\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return negative, positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11068e8db61f984e7c6cd121dc23b5d5",
     "grade": true,
     "grade_id": "A-1-1-public",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 1 point\n",
    "negative, positive = word_start_with_dis()\n",
    "assert positive > 0\n",
    "assert negative > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "116abdbf207d7e6fa8f7025199a65cc4",
     "grade": true,
     "grade_id": "A-1-1-private",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private tests here\n",
    "# 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95cf6fcb2d62e545c4e405d3dfe15188",
     "grade": false,
     "grade_id": "cell-3774c090c0dae0ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2) Conditional probability (2 points)\n",
    "\n",
    "Your task: ***Calculate the `probability` of a word having a `negative sentiment`, given that it `starts with \"dis\"` over the whole lexicon***\n",
    "\n",
    "To estimate the probability, we need to divide the number of all `negative words` starting with `\"dis\"` over the sum of `all words` (`positive and negative`) starting with `\"dis\"`\n",
    "\n",
    "$$P(\\text{negative | starts with \"dis\"}) = \\frac{\\text{# negative words starting with \"dis\"}}{\\text{# positive & negative words starting with \"dis\"}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "938b1b47fbe5edc4cff956d347f7a2b0",
     "grade": false,
     "grade_id": "A-1-2-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cond_probability(negative: int, positive: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the probability of a word having a negative sentiment \n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    negative: int\n",
    "        Number of negative words\n",
    "    positive: int\n",
    "        Number of positive words\n",
    "        \n",
    "    return: float\n",
    "        Probability of having a negative sentiment\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28df7435027c67e8b124dee828bd6f70",
     "grade": true,
     "grade_id": "A-1-2-pulic",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 1 point\n",
    "result = cond_probability(negative, positive)\n",
    "assert 0 < result < 1, \"A probability ranges from 0 to 1\"\n",
    "assert isinstance(result, float), \"A probability is a float number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e1fc85a0e7fbd7e323e620a21befeff",
     "grade": true,
     "grade_id": "A-1-2-private",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private tests here\n",
    "# 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6cfa556578b0b6f25360fcd9d7d1eea",
     "grade": false,
     "grade_id": "A-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# A.2 Text Generation (11 points)\n",
    "In Lecture 4, we show how to generate a sentence using $n$-gram language models built from `english-kjv.txt` corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "871ff9299a9b222c1bcf73a94c08c2cd",
     "grade": false,
     "grade_id": "cell-3cc592583b801cde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edf6cd25dda8360d09017cab49914408",
     "grade": false,
     "grade_id": "cell-db7c906956d42cb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load words from `english-kjv.txt`\n",
    "text = nltk.corpus.genesis.words(\"english-kjv.txt\")\n",
    "# Get bigrams from the text\n",
    "bigrams_obj = nltk.bigrams(text)\n",
    "# Use each word as the given condition for the following word \n",
    "# and create conditional frequency distribution \n",
    "cfd = nltk.ConditionalFreqDist(bigrams_obj)\n",
    "\n",
    "\n",
    "# Function from Lecture 4\n",
    "def generate_sentence(\n",
    "    bigram_model: nltk.ConditionalFreqDist,\n",
    "    start_word: str, \n",
    "    number_of_words=20\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate sentence from the given prompt and bigram model  \n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    bigram_model: ConditionalFreqDist\n",
    "        Bigram model in form of conditional frequency distribution \n",
    "    start_word: str\n",
    "        Prompt to generate sentence\n",
    "    number_of_words: int\n",
    "        Length of the generated sequence\n",
    "    \n",
    "    return: string\n",
    "        A generated text with given bigram language model, start prompt and number of generated words\n",
    "    \"\"\"\n",
    "    prev_word = start_word\n",
    "    words = [prev_word]\n",
    "    for i in range(number_of_words):\n",
    "        # select the next word with the highest probability\n",
    "        word = bigram_model[prev_word].max()\n",
    "        prev_word = word\n",
    "        words.append(word)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6544a0ffaf37a2846b69e5b7210cc4e4",
     "grade": false,
     "grade_id": "cell-140bd7299e1d59bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's try to generate a sentence from the word `living`\n",
    "generate_sentence(cfd, \"living\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a102c40db7c60a69b90db07cadb93ae6",
     "grade": false,
     "grade_id": "cell-4d71e860de835143",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1) Build bigram model from a corpus (2 points)\n",
    "\n",
    "In this task, we are going to use the `austen-persuasion.txt` from `gutenberg` corpus.\n",
    "\n",
    "Your task: **Load words from the corpus and build a bigram model from it using `nltk.ConditionalFreqDist`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc04c90372e7314bb6450c4efcd52fe4",
     "grade": false,
     "grade_id": "cell-9a4481bcf1ad467a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Download 'gutenberg' corpus if you haven't already\n",
    "## nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56af9f7d48a6ca1c3d935d21b692cb61",
     "grade": false,
     "grade_id": "A-2-1-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def build_bigram_models_from_austen_persuasion():\n",
    "    \"\"\"\n",
    "    Build a bigram model from words in austen-persuasion.txt\n",
    "    \n",
    "    return: ConditionalFreqDist\n",
    "        Bigram model with words from austen-persuasion.txt\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return bigram_model\n",
    "\n",
    "bigram_model = build_bigram_models_from_austen_persuasion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31a4eb6dfd335167bb2483c8887ca12e",
     "grade": false,
     "grade_id": "cell-7cf799fd44ad2c7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's try to generate a sentence with the new bigram model\n",
    "start_word = \"We\"\n",
    "generated_text_w_highest_prob = generate_sentence(bigram_model, start_word)\n",
    "print(\"Generated sentence by selecting highest probability: \\n{}\".format(\n",
    "    generated_text_w_highest_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d95eb391f178194b4f9253fb873aba35",
     "grade": true,
     "grade_id": "A-2-1-public",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 1 point\n",
    "start_word = \"We\"\n",
    "generated_text_w_highest_prob = generate_sentence(bigram_model, start_word)\n",
    "expected_gen_text = \"We are not be a very much to be a very much to be a very much to be a very\"\n",
    "assert generated_text_w_highest_prob == expected_gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0ad0ab7dccd5d6e55779b42abc5a378",
     "grade": true,
     "grade_id": "A-2-1-private",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private tests here\n",
    "# 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f55c8cf08f7c96e553efb4a0a30d76ce",
     "grade": false,
     "grade_id": "cell-ed2d890dc474b906",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2) Generate sentence from most common words (9 points)\n",
    "\n",
    "Your task: **generate a sentence of `20 words` from a given `start_word` by randomly selecting the next word from the `5 most common words` that may `occur next`, using the bigram model built on `austen-persuasion.txt`**\n",
    "\n",
    "- Words for our bi-grams model are taken from `austen-persuasion.txt` text in `Gutenberg` corpus. \n",
    "- The generated text should have exactly `20` words/tokens.\n",
    "\n",
    "**Hint:** \n",
    "- `ConditionalFreqDist.most_common(n: int)` takes a number `n` as input and returns a list of `n` most common words and their counts \\[(word_1, n_1), (word_2, n_2), ...\\]\n",
    "- `random.choice` takes input as a list of tokens (top 5 possible tokens given the previous token) and randomly selects one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54f0e6c1301898df88db6dceb6d33a41",
     "grade": false,
     "grade_id": "A-2-2-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_sentence_from_most_common_words(\n",
    "        bigram_model: nltk.ConditionalFreqDist,\n",
    "        start_word: str,\n",
    "        number_of_words=20,\n",
    "        number_of_most_common_words=5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a sequence of tokens with given start token/word using ConditionalFreqDist()\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    start_word: string\n",
    "        a word/token\n",
    "        \n",
    "    return: string\n",
    "        a generated text from exactly 20 tokens \n",
    "    \"\"\"    \n",
    "    # Initialize the sentence with the start word\n",
    "    prev_word = start_word\n",
    "    list_word = [prev_word]\n",
    "    \n",
    "    # Find top 5 words that can possibly follow the current word\n",
    "    # and randomly select one \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # returns the generated sentence \n",
    "    return \" \".join(list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddbd7355c8f5f1faf21d8549132dce80",
     "grade": false,
     "grade_id": "cell-10fdc078afcecd37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Try to print the generated sentence here\n",
    "start_word = \"We\"\n",
    "generated_text_w_most_common = generate_sentence_from_most_common_words(bigram_model, start_word)\n",
    "print(\"Generated sentence by randomly selecting most common next words: \\n{}\".format(\n",
    "    generated_text_w_most_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31153a9ddec57b089378a49a9b347a18",
     "grade": true,
     "grade_id": "A-2-2-public-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# + 1 start_word\n",
    "# 1 point\n",
    "assert len(nltk.tokenize.word_tokenize(generated_text_w_most_common)) == 20 + 1, \\\n",
    "    \"Check whether the function generates 20 words after the start_word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ebe77da4c42d67e12677b56af7c3ba3",
     "grade": true,
     "grade_id": "A-2-2-public-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 1 point\n",
    "assert generated_text_w_highest_prob != generated_text_w_most_common, \\\n",
    "    \"Two generate functions should return different sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77c7c4ad0b7975a637eae4c7904ee661",
     "grade": true,
     "grade_id": "A-2-2-private-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2043e27bf9e37f68a9e6819e38acb29c",
     "grade": true,
     "grade_id": "A-2-2-private-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# We set the random seed to 20 for reproducibility\n",
    "random.seed(20)\n",
    "start_word = \"We\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07220073faf3c54931a6cc1b7af3f0f8",
     "grade": true,
     "grade_id": "A-2-2-private-3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# This will test whether the solution use \n",
    "# the input values of `number_of_words`\n",
    "# and `number_of_most_common_words`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c48e8c64ffabf16f3076b0d9a341b066",
     "grade": false,
     "grade_id": "cell-a7a627359c624b00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A.3 Regular Expression (10 points)\n",
    "\n",
    "Create a function `trigrams_regex(tokens)`, which finds from a given text all sequences of 3-grams that \n",
    "1. have an article, \n",
    "2. a potential adjective ending with -ing \n",
    "3. and a following word by using **regular expressions only**. \n",
    "Make sure that the third element of the 3-gram is a word (not punctuation). \n",
    "\n",
    "The function should take a list of tokens as an argument and return a list of all found n-grams. \n",
    "\n",
    "Try the function on `austen-persuasion.txt` text from the Gutenberg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72de0f32bc19d14f231569dc4ef28de9",
     "grade": false,
     "grade_id": "cell-26aa7c009c0255ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "## Download 'gutenberg' corpus if you haven't already\n",
    "## nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27df4a86e0f75fb8f92c46db0fc9fe10",
     "grade": false,
     "grade_id": "cell-d2d3a2e9370640a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`nltk.TokenSearcher` is class that makes it easier to use regular expressions to search over tokenized strings. The tokenized string is converted to a string where tokens are marked with angle brackets – e.g., '<the><window><is><still><open>'. The regular expression passed to the `findall()` method is modified to treat angle brackets as non-capturing parentheses, in addition to matching the token boundaries; and to have '.' not match the angle brackets. Try to run examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85e42088a4ea20efc3c4c007e154431c",
     "grade": false,
     "grade_id": "cell-f11e882e62000a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.text import TokenSearcher\n",
    "from nltk.book import text1, text5, text9\n",
    "print(text5.findall(\"<.*><.*><bro>\"))\n",
    "print(text1.findall(\"<a>(<.*>)<man>\"))\n",
    "print(text9.findall(\"<th.*>{3,}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a50020423d0fc73cc0aa0dae896fa950",
     "grade": false,
     "grade_id": "cell-91815858c8aa4c45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "token_searcher = nltk.text.TokenSearcher(gutenberg.words('austen-persuasion.txt'))\n",
    "found_tokens = token_searcher.findall(r\"<\\d+><[A-Za-z]+><[A-Za-z]+><[A-Za-z]+>\")\n",
    "found_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5611d1acfb1d64980c924b3875712b3",
     "grade": false,
     "grade_id": "cell-364df116f2346f5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`token_searcher.findall(r\"<\\d+><[A-Za-z]+><[A-Za-z]+><[A-Za-z]+>\")` returns a list of list where the innere list consists of 4 matched tokens as follows:\n",
    "\n",
    "- 1st token `<d+>`: matches a digit (equivalent to `[0-9]`) and matches the previous digit at least one to unlimited times\n",
    "- 2nd, 3rd, 4th token `<[A-Za-z]+>`: matches a single character in the range between `[A-Z]` or `[a-z]` and matches the previous character at least one to unlimited times\n",
    "\n",
    "With this pattern, we can search for the chapter number and exactly the 3 words that follow it in `'austen-persuasion.txt'`. Since chapter 2 does not match, this means that the first 3 words of the chapter do not belong to the Latin alphabet, but with a punctuation or a character with diacritic for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb16cc169dd778c2f4ae2bb41fc77374",
     "grade": false,
     "grade_id": "A-3-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def trigrams_regex(tokens: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Given a given list of tokens, find all 3-grams from the list that \n",
    "    1. starts with an article \n",
    "    2. followed by a potential adjective ending with -ing\n",
    "    3. and any following word\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    tokens: list of tokens/word\n",
    "        e.g., from austen-persuasion.txt\n",
    "    return: list of list of sequence of 3-grams\n",
    "        e.g., [[a, ding, word], [an, sing, token]]\n",
    "    \"\"\"\n",
    "    bracketed = nltk.text.TokenSearcher(tokens)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "079033b86a0e0ef48b1fe46add4486a2",
     "grade": true,
     "grade_id": "A-3-public",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 2 points\n",
    "trigrams = trigrams_regex(gutenberg.words('austen-persuasion.txt'))\n",
    "assert trigrams[0] == ['the', 'beginning', 'and']\n",
    "assert sum([len(t) for t in trigrams]) / len(trigrams) == 3, \"We are looking for trigrams (3-grams)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "594d5e9587ad4c21c3d3cfe627f4de69",
     "grade": true,
     "grade_id": "A-3-private",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private tests here\n",
    "# 4 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bde23a03c031e87ae1514d57afd127f3",
     "grade": false,
     "grade_id": "B-",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part B: spaCy (10 point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98ddb8128be0035d39f062c9b9a51578",
     "grade": false,
     "grade_id": "B-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### B.1 Explore real dataset - 10 Points\n",
    "In this exercise, you will work with a real dataset of public english-language tweets for the keyword 'lockdown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10fd7539c77a02c17acdb33c2551675d",
     "grade": false,
     "grade_id": "B-1-intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "## download the language model if you haven't already\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c6947adf02e8e1650ca1733b294a2e7",
     "grade": false,
     "grade_id": "B-1-1-intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To get started, you will have to read the dataset from the provided `tweets.txt` file. Each line in this file represents a single tweet. You will need to open and read the file before starting the other subtasks. \n",
    "\n",
    "#### 1) Tokenize each tweet in the dataset. Use spaCy to solve this task. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c1e00cf309e6bb6138101c205aea66c",
     "grade": false,
     "grade_id": "B-1-1-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_data(data_name: str) -> spacy.tokens.doc.Doc:\n",
    "    \"\"\"\n",
    "    Read the file name and tokenize each tweet\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    data_name: string\n",
    "        name of the file\n",
    "    return: spacy.tokens.doc.Doc\n",
    "        object from spaCy that each token can be iterated\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7158c6f187d648b8c27b4ea8fdf4518c",
     "grade": false,
     "grade_id": "B-1-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "doc = read_data('tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4cbaa2af879965ad3d46890e8af60d6",
     "grade": true,
     "grade_id": "B-1-1-public",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 1 point\n",
    "assert doc[0].text == 'Nothing'\n",
    "assert isinstance(doc, spacy.tokens.doc.Doc), \"read_data should return `spacy.tokens.doc.Doc` type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d8547e262baafb99f90d74543b16115",
     "grade": true,
     "grade_id": "B-1-1-private",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private tests here\n",
    "# 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd1a602827d5e89acecb4e10659e2c58",
     "grade": false,
     "grade_id": "B-1-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2) Implement the function `occurence_lowercase`.  (8 points)\n",
    "\n",
    "The function should calculate the (absolute) number of occurrences of each token that is in lowercase. \n",
    "Apply the function to our dataset of tweets and return the result (i.e. 'token: occurence') in descending order. Use spaCy to identify lowercased tokens. (7 points)\n",
    "\n",
    "**Hint**: Do not lowercase all tokens, instead identify and count all already lowercased tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8eea7d4bfc5c525cee69c3890c55990",
     "grade": false,
     "grade_id": "B-1-2-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def occurence_lowercase(data) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Counts occurences of all lowercased tokens.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    data: array-like object\n",
    "        containing tokenized tweets from previous subtask\n",
    "    return: dict-like object         \n",
    "        object with tokens and their counts\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de5907e3b15e963d1519f0293f68f673",
     "grade": true,
     "grade_id": "B-1-2-public",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# 2 points\n",
    "occ = occurence_lowercase(doc)\n",
    "assert occ['the'] == 1111\n",
    "assert occ['but'] == 145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af9ed9c337713c342a7ccb9865d99d6a",
     "grade": true,
     "grade_id": "B-1-2-private",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private tests here\n",
    "# 6 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
